diff --git a/ansible/playbooks/identity-deploy-and-handover.yml b/ansible/playbooks/identity-deploy-and-handover.yml
index 79cb02b..ee3b507 100644
--- a/ansible/playbooks/identity-deploy-and-handover.yml
+++ b/ansible/playbooks/identity-deploy-and-handover.yml
@@ -21,6 +21,7 @@
     keycloak_manifest: "{{ repo_root }}/manifests/keycloak.yaml"
     keycloak_helm_chart: "codecentric/keycloak"
     keycloak_values_file: "{{ repo_root }}/helm/keycloak-values.yaml"
+    postgresql_manifest: "{{ repo_root }}/manifests/identity/postgresql-statefulset.yaml"
     
     # Storage manifests for Keycloak PostgreSQL
     storage_class_manifest: "{{ repo_root }}/manifests/identity/storage-class-manual.yaml"
@@ -64,11 +65,11 @@
     # Increased from 180s to 300s to accommodate PostgreSQL initialization on slower systems
     rollout_wait_timeout: 120
     
-    # PostgreSQL image configuration
+    # PostgreSQL image configuration  
     postgresql_image_registry: docker.io
     postgresql_image_repository: postgres
     postgresql_image_tag: "11"
-    # PostgreSQL user/group IDs for the official postgres image (UID/GID 999)
+    # PostgreSQL user/group IDs for the postgres:11 image (UID/GID 999)
     # This must match the securityContext in keycloak-values.yaml
     postgresql_uid: "999"
     postgresql_gid: "999"
@@ -99,6 +100,7 @@
           register: infra_node_detect
           changed_when: false
           failed_when: false
+          become: true
 
         - name: Fallback to first schedulable node if no control-plane found
           shell: |
@@ -106,6 +108,7 @@
           register: infra_node_fallback
           changed_when: false
           when: infra_node_detect.stdout == ""
+          become: true
 
         - name: Set infra_node variable
           set_fact:
@@ -130,6 +133,7 @@
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       changed_when: false
+      become: true
 
     - name: Create identity data directories for persistent storage
       file:
@@ -151,6 +155,7 @@
       register: uncordon_result
       failed_when: false
       changed_when: false
+      become: true
 
     - name: Deploy StorageClass for Keycloak PostgreSQL (idempotent)
       shell: kubectl apply -f {{ storage_class_manifest }}
@@ -158,6 +163,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: storage_class_apply
       changed_when: "'created' in storage_class_apply.stdout or 'configured' in storage_class_apply.stdout"
+      become: true
 
     - name: Deploy PersistentVolume for Keycloak PostgreSQL (idempotent)
       shell: kubectl apply -f {{ keycloak_pv_manifest }}
@@ -165,6 +171,17 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_pv_apply
       changed_when: "'created' in keycloak_pv_apply.stdout or 'configured' in keycloak_pv_apply.stdout"
+      become: true
+
+    - name: Clear claimRef from Keycloak PostgreSQL PV if it's Released
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch pv keycloak-postgresql-pv 
+        --type=json -p='[{"op":"remove","path":"/spec/claimRef"}]'
+      when: keycloak_pv_apply.rc == 0
+      register: pv_claim_clear
+      failed_when: false
+      changed_when: "'patched' in pv_claim_clear.stdout"
+      become: true
 
     - name: Optionally fix hostPath ownership for PostgreSQL via privileged Job
       when: (enable_postgres_chown | default(false) | bool) or (identity_force_replace | default(false) | bool)
@@ -209,6 +226,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: chown_job_apply
           changed_when: "'created' in chown_job_apply.stdout or 'configured' in chown_job_apply.stdout"
+          become: true
 
         - name: Wait for chown Job completion
           shell: kubectl -n {{ namespace_identity }} wait --for=condition=complete job/postgres-chown-job --timeout=120s
@@ -216,6 +234,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: chown_job_wait
           failed_when: chown_job_wait.rc != 0
+          become: true
 
         - name: Delete chown Job
           shell: kubectl -n {{ namespace_identity }} delete job/postgres-chown-job --ignore-not-found
@@ -223,6 +242,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           changed_when: false
           failed_when: false
+          become: true
 
     - name: Optionally fix hostPath ownership for FreeIPA via privileged Job
       when: (enable_freeipa_chown | default(false) | bool) or (identity_force_replace | default(false) | bool)
@@ -267,6 +287,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: freeipa_chown_job_apply
           changed_when: "'created' in freeipa_chown_job_apply.stdout or 'configured' in freeipa_chown_job_apply.stdout"
+          become: true
 
         - name: Wait for FreeIPA chown Job completion
           shell: kubectl -n {{ namespace_identity }} wait --for=condition=complete job/freeipa-chown-job --timeout=120s
@@ -274,6 +295,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: freeipa_chown_job_wait
           failed_when: freeipa_chown_job_wait.rc != 0
+          become: true
 
         - name: Delete FreeIPA chown Job
           shell: kubectl -n {{ namespace_identity }} delete job/freeipa-chown-job --ignore-not-found
@@ -281,23 +303,25 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           changed_when: false
           failed_when: false
+          become: true
 
     - name: Check if FreeIPA manifest exists
       stat:
         path: "{{ freeipa_manifest }}"
       register: freeipa_manifest_stat
 
+    - name: Ensure backup directory exists upfront (root-owned)
+      file:
+        path: "{{ backup_dir }}"
+        state: directory
+        owner: root
+        group: root
+        mode: '0700'
+      become: true
+
     - name: "Optional: Backup and remove existing identity pods before redeploy (controlled)"
       when: identity_force_replace | default(false) | bool and identity_backup_before_replace | default(true) | bool
       block:
-        - name: Create backup directory on controller (root-owned)
-          file:
-            path: "{{ backup_dir }}"
-            state: directory
-            owner: root
-            group: root
-            mode: '0700'
-          become: true
 
         - name: Get current timestamp for backup filename
           shell: date -u +%Y%m%dT%H%M%SZ
@@ -366,28 +390,33 @@
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts freeipa --replicas=0 --timeout=60s || true
           register: scale_down_freeipa
           failed_when: false
+          become: true
 
         - name: Scale down Keycloak StatefulSet to 0 (if present)
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak --replicas=0 --timeout=60s || true
           register: scale_down_keycloak
           failed_when: false
+          become: true
 
         - name: Scale down PostgreSQL StatefulSet to 0 (if present)
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=0 --timeout=60s || true
           register: scale_down_postgres
           failed_when: false
+          become: true
 
         - name: Delete any remaining identity pods (force removal)
           shell: >-
             KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} delete pod --all --ignore-not-found --grace-period=10 --timeout=60s || true
           register: delete_identity_pods
           failed_when: false
+          become: true
 
         - name: Wait for identity pods to be fully removed
           shell: >-
             KUBECONFIG=/etc/kubernetes/admin.conf bash -c "for i in {1..60}; do if [ -z \"$(kubectl -n {{ namespace_identity }} get pods -o name 2>/dev/null)\" ]; then exit 0; fi; sleep 1; done; exit 1"
           register: wait_for_pod_removal
           failed_when: wait_for_pod_removal.rc != 0
+          become: true
 
     - name: Deploy FreeIPA from manifest (if available)
       shell: kubectl apply -f {{ freeipa_manifest }}
@@ -396,6 +425,17 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: freeipa_apply
       changed_when: "'created' in freeipa_apply.stdout or 'configured' in freeipa_apply.stdout"
+      become: true
+
+    - name: Clear claimRef from FreeIPA PV if it's Released
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch pv freeipa-data-pv 
+        --type=json -p='[{"op":"remove","path":"/spec/claimRef"}]'
+      when: freeipa_apply is defined and freeipa_apply.rc == 0
+      register: freeipa_pv_claim_clear
+      failed_when: false
+      changed_when: "'patched' in freeipa_pv_claim_clear.stdout"
+      become: true
 
     - name: Set nodeSelector for FreeIPA StatefulSet
       shell: |
@@ -405,6 +445,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: freeipa_node_selector
       failed_when: false
+      become: true
 
     - name: Inform about missing FreeIPA manifest
       debug:
@@ -423,6 +464,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_apply
       changed_when: "'created' in keycloak_apply.stdout or 'configured' in keycloak_apply.stdout"
+      become: true
 
     - name: Check if Keycloak values file has placeholder passwords
       shell: |
@@ -468,12 +510,20 @@
           ============================================================
       when: freeipa_password_check is defined and freeipa_password_check.stdout != ""
 
+    - name: Deploy PostgreSQL StatefulSet for Keycloak
+      shell: kubectl apply -f {{ postgresql_manifest }}
+      environment:
+        KUBECONFIG: /etc/kubernetes/admin.conf
+      register: postgresql_apply
+      changed_when: "'created' in postgresql_apply.stdout or 'configured' in postgresql_apply.stdout"
+      become: true
+
     - name: Check if Keycloak values file exists
       stat:
         path: "{{ keycloak_values_file }}"
       register: keycloak_values_stat
 
-    - name: Install/upgrade Keycloak via Helm
+    - name: Install/upgrade Keycloak via Helm (PostgreSQL disabled)
       shell: >-
         helm repo add codecentric https://codecentric.github.io/helm-charts >/dev/null 2>&1 || true;
         helm repo update >/dev/null 2>&1;
@@ -481,18 +531,38 @@
         -n {{ namespace_identity }} 
         -f {{ keycloak_values_file }} 
         --create-namespace 
-        --set postgresql.image.registry={{ postgresql_image_registry }}
-        --set postgresql.image.repository={{ postgresql_image_repository }}
-        --set postgresql.image.tag={{ postgresql_image_tag }}
-        --set postgresql.image.pullPolicy=IfNotPresent
+        --set postgresql.enabled=false
         --set keycloak.nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
-        --set postgresql.nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
       when: keycloak_values_stat.stat.exists
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_helm_install
+      become: true
 
+    - name: Patch Keycloak StatefulSet with PostgreSQL environment variables
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch statefulset keycloak -n {{ namespace_identity }} 
+        --type=json -p='[{"op":"add","path":"/spec/template/spec/containers/0/env","value":[
+        {"name":"DB_VENDOR","value":"postgres"},
+        {"name":"DB_ADDR","value":"keycloak-postgresql"},
+        {"name":"DB_PORT","value":"5432"},
+        {"name":"DB_DATABASE","value":"keycloak"},
+        {"name":"DB_USER","value":"keycloak"},
+        {"name":"DB_PASSWORD","value":"CHANGEME_DB_PASSWORD"}
+        ]}]'
+      register: keycloak_env_patch
+      failed_when: false
+      changed_when: "'patched' in keycloak_env_patch.stdout"
+      become: true
 
+    - name: Patch Keycloak StatefulSet with nodeSelector for masternode
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch statefulset keycloak -n {{ namespace_identity }} 
+        -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"{{ infra_node }}"}}}}}'
+      register: keycloak_node_patch
+      failed_when: false
+      changed_when: "'patched' in keycloak_node_patch.stdout"
+      become: true
 
     - name: Wait for Keycloak PostgreSQL PVC to be created
       shell: >-
@@ -502,6 +572,7 @@
       retries: 30
       delay: 2
       failed_when: false
+      become: true
 
     - name: Check Keycloak PostgreSQL PVC status
       shell: >-
@@ -509,6 +580,7 @@
       register: keycloak_postgres_pvc_phase
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Find Available PVs labeled for Keycloak PostgreSQL
       shell: >-
@@ -516,6 +588,7 @@
       register: keycloak_postgres_available_pvs
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Bind an Available PV to Keycloak PVC when PVC is Pending
       shell: >-
@@ -528,6 +601,7 @@
       register: keycloak_pv_bind
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
+      become: true
 
     - name: Log PV binding result
       debug:
@@ -540,6 +614,7 @@
       register: keycloak_postgres_pvc_verify
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Display PVC status
       debug:
@@ -559,6 +634,7 @@
             -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
           register: keycloak_pg_rollout
           failed_when: keycloak_pg_rollout.rc != 0
+          become: true
 
       rescue:
         - name: Handle PostgreSQL rollout timeout
@@ -579,6 +655,12 @@
                   kubectl -n {{ namespace_identity }} describe $pod || true
                 done
                 echo ""
+                echo "=== PostgreSQL Logs ==="
+                for pod in $(kubectl -n {{ namespace_identity }} get pods -l app.kubernetes.io/name=postgresql -o name); do
+                  echo "--- Logs: $pod ---"
+                  kubectl -n {{ namespace_identity }} logs $pod --all-containers --tail=100 || true
+                done
+                echo ""
                 echo "=== PVC Status ==="
                 kubectl -n {{ namespace_identity }} get pvc -o yaml || true
                 echo ""
@@ -590,6 +672,7 @@
               register: pg_diagnostics
               environment:
                 KUBECONFIG: /etc/kubernetes/admin.conf
+              become: true
 
             - name: Save PostgreSQL diagnostics
               copy:
@@ -621,10 +704,12 @@
                 - name: Scale down PostgreSQL StatefulSet
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=0 --timeout=60s
                   failed_when: false
+                  become: true
 
                 - name: Delete PostgreSQL pods
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} delete pod -l app.kubernetes.io/name=postgresql --ignore-not-found --grace-period=10
                   failed_when: false
+                  become: true
 
                 - name: Wait for pod removal
                   pause:
@@ -633,12 +718,14 @@
                 - name: Scale up PostgreSQL StatefulSet
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=1
                   failed_when: false
+                  become: true
 
                 - name: Wait for PostgreSQL rollout after recovery
                   shell: >-
                     KUBECONFIG=/etc/kubernetes/admin.conf kubectl rollout status statefulset/keycloak-postgresql 
                     -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
                   register: pg_rollout_after_recovery
+                  become: true
 
             - name: Fail with diagnostics notice if recovery not attempted
               fail:
@@ -653,6 +740,7 @@
             -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
           register: keycloak_rollout
           failed_when: keycloak_rollout.rc != 0
+          become: true
 
       rescue:
         - name: Handle Keycloak rollout timeout
@@ -673,11 +761,18 @@
                   kubectl -n {{ namespace_identity }} describe $pod || true
                 done
                 echo ""
+                echo "=== Keycloak Logs ==="
+                for pod in $(kubectl -n {{ namespace_identity }} get pods -l app.kubernetes.io/name=keycloak -o name); do
+                  echo "--- Logs: $pod ---"
+                  kubectl -n {{ namespace_identity }} logs $pod --all-containers --tail=100 || true
+                done
+                echo ""
                 echo "=== Events ==="
                 kubectl -n {{ namespace_identity }} get events --sort-by=.metadata.creationTimestamp || true
               register: kc_diagnostics
               environment:
                 KUBECONFIG: /etc/kubernetes/admin.conf
+              become: true
 
             - name: Save Keycloak diagnostics
               copy:
@@ -699,6 +794,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: certmgr_crds
       changed_when: "'created' in certmgr_crds.stdout or 'configured' in certmgr_crds.stdout"
+      become: true
 
     - name: Add cert-manager Helm repo
       shell: |
@@ -715,6 +811,27 @@
         --set installCRDs=false
         --set nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
       register: certmgr_helm
+      become: true
+
+    - name: Patch cert-manager cainjector with nodeSelector for masternode
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch deployment cert-manager-cainjector 
+        -n {{ namespace_cert_manager }} 
+        -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"{{ infra_node }}"}}}}}'
+      register: certmgr_cainjector_patch
+      failed_when: false
+      changed_when: "'patched' in certmgr_cainjector_patch.stdout"
+      become: true
+
+    - name: Patch cert-manager webhook with nodeSelector for masternode
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch deployment cert-manager-webhook 
+        -n {{ namespace_cert_manager }} 
+        -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"{{ infra_node }}"}}}}}'
+      register: certmgr_webhook_patch
+      failed_when: false
+      changed_when: "'patched' in certmgr_webhook_patch.stdout"
+      become: true
 
     - name: Wait for cert-manager deployments to be ready (with diagnostics on failure)
       block:
@@ -725,6 +842,7 @@
           failed_when: certmgr_rollout_controller.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Wait for cert-manager webhook rollout
           shell: >-
@@ -733,6 +851,7 @@
           failed_when: certmgr_rollout_webhook.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Wait for cert-manager cainjector rollout
           shell: >-
@@ -741,6 +860,7 @@
           failed_when: certmgr_rollout_cainjector.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
       rescue:
         - name: Get UTC timestamp for diagnostics filename
@@ -752,6 +872,7 @@
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_cert_manager }} get pods -o wide
           register: certmgr_pods
           changed_when: false
+          become: true
 
         - name: Describe cert-manager namespace pods
           shell: |
@@ -763,22 +884,25 @@
           changed_when: false
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
-        - name: Collect cert-manager webhook logs (best-effort)
+        - name: Collect cert-manager logs from all pods
           shell: |
-            set -e; for p in $(kubectl -n {{ namespace_cert_manager }} get pods -l app.kubernetes.io/component=webhook -o name 2>/dev/null || true); do
-              echo "=== logs $p ===";
-              kubectl -n {{ namespace_cert_manager }} logs $p --all-containers || true;
+            for p in $(kubectl -n {{ namespace_cert_manager }} get pods -o name 2>/dev/null || true); do
+              echo "=== Logs: $p ===";
+              kubectl -n {{ namespace_cert_manager }} logs $p --all-containers --tail=100 || true;
             done
           register: certmgr_logs
           changed_when: false
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Collect events in cert-manager namespace
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_cert_manager }} get events --sort-by=.metadata.creationTimestamp
           register: certmgr_events
           changed_when: false
+          become: true
 
         - name: Save cert-manager diagnostics to root backup dir
           copy:
@@ -807,19 +931,21 @@
       register: crd_check
       failed_when: crd_check.rc != 0
       changed_when: false
+      become: true
 
     - name: Ensure CA cert file exists
       stat:
         path: "{{ ca_cert_src }}"
       register: ca_cert_stat
 
-    - name: Ensure backup directory exists (root-owned)
+    - name: Ensure backup directory exists (root-owned) before CA backup
       file:
         path: "{{ backup_dir }}"
         state: directory
         owner: root
         group: root
         mode: '0700'
+      become: true
 
     - name: Backup CA material to root backup (cert and key if present)
       shell: >-
@@ -831,12 +957,16 @@
       register: backup_run
       changed_when: backup_run.rc == 0
 
-    - name: Create or update Kubernetes Secret with CA
+    - name: Create or update Kubernetes Secret with CA (tls.crt and tls.key format for cert-manager)
       shell: >-
-        kubectl create secret generic {{ secret_name }} --namespace {{ namespace_cert_manager }} --from-file=ca.crt={{ ca_cert_src }} --dry-run=client -o yaml | kubectl apply -f -
+        kubectl create secret generic {{ secret_name }} --namespace {{ namespace_cert_manager }} 
+        --from-file=tls.crt={{ ca_cert_src }} 
+        --from-file=tls.key={{ ca_key_src }} 
+        --dry-run=client -o yaml | kubectl apply -f -
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: secret_apply
+      become: true
 
     - name: Render ClusterIssuer template
       template:
@@ -848,6 +978,150 @@
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: issuer_apply
+      become: true
+
+    - name: Deploy Keycloak NodePort Service for desktop access
+      shell: >-
+        cat <<EOF | kubectl apply -f -
+        apiVersion: v1
+        kind: Service
+        metadata:
+          name: keycloak-nodeport
+          namespace: {{ namespace_identity }}
+          labels:
+            app: keycloak
+            component: nodeport
+          annotations:
+            description: "NodePort service for desktop access to Keycloak"
+        spec:
+          type: NodePort
+          ports:
+            - name: http
+              port: 80
+              targetPort: 8080
+              nodePort: 30080
+              protocol: TCP
+            - name: https
+              port: 8443
+              targetPort: 8443
+              nodePort: 30443
+              protocol: TCP
+          selector:
+            app.kubernetes.io/name: keycloak
+        EOF
+      environment:
+        KUBECONFIG: /etc/kubernetes/admin.conf
+      register: keycloak_nodeport_apply
+      changed_when: "'created' in keycloak_nodeport_apply.stdout or 'configured' in keycloak_nodeport_apply.stdout"
+      become: true
+
+    - name: Setup Keycloak admin user and desktop access
+      block:
+        - name: Generate secure admin password if not provided
+          shell: openssl rand -base64 32
+          register: generated_admin_password
+          when: keycloak_admin_password is not defined or keycloak_admin_password == ''
+          changed_when: false
+
+        - name: Set admin password variable
+          set_fact:
+            admin_password: "{{ keycloak_admin_password | default(generated_admin_password.stdout) }}"
+
+        - name: Wait for Keycloak pod to be ready
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl wait --for=condition=ready 
+            pod/keycloak-0 -n {{ namespace_identity }} --timeout=300s
+          register: keycloak_ready
+          failed_when: keycloak_ready.rc != 0
+          become: true
+
+        - name: Check if admin user exists in Keycloak
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n {{ namespace_identity }} keycloak-0 -- 
+            /opt/jboss/keycloak/bin/kcadm.sh get users -r master --fields username 2>/dev/null | 
+            grep -c '"admin"' || echo "0"
+          register: admin_user_check
+          changed_when: false
+          failed_when: false
+          become: true
+
+        - name: Create Keycloak admin user
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n {{ namespace_identity }} keycloak-0 -- 
+            /opt/jboss/keycloak/bin/add-user-keycloak.sh 
+            -r master -u admin -p "{{ admin_password }}"
+          when: admin_user_check.stdout == "0"
+          register: admin_user_create
+          become: true
+
+        - name: Restart Keycloak pod to apply admin user
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl delete pod keycloak-0 -n {{ namespace_identity }}
+          when: admin_user_check.stdout == "0" and admin_user_create.rc == 0
+          register: keycloak_restart
+          become: true
+
+        - name: Wait for Keycloak pod restart
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl wait --for=condition=ready 
+            pod/keycloak-0 -n {{ namespace_identity }} --timeout=300s
+          when: keycloak_restart is defined and keycloak_restart.rc == 0
+          register: keycloak_reready
+          become: true
+
+        - name: Get cluster node IPs for access URLs
+          shell: >-
+            KUBECONFIG=/etc/kubernetes/admin.conf kubectl get nodes 
+            -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
+          register: node_ips
+          changed_when: false
+          become: true
+
+        - name: Save Keycloak admin credentials to secure location
+          copy:
+            dest: "{{ backup_dir }}/keycloak-admin-credentials.txt"
+            content: |
+              Keycloak Admin Credentials
+              Generated: {{ ansible_date_time.iso8601 }}
+              
+              Username: admin
+              Password: {{ admin_password }}
+              
+              Access URLs (from desktop):
+              {% for ip in node_ips.stdout.split() %}
+              - http://{{ ip }}:30080/auth
+              {% endfor %}
+              
+              Admin Console:
+              {% for ip in node_ips.stdout.split() %}
+              - http://{{ ip }}:30080/auth/admin
+              {% endfor %}
+            owner: root
+            group: root
+            mode: '0600'
+          become: true
+
+        - name: Display Keycloak access information
+          debug:
+            msg: |
+              ============================================================
+              Keycloak Admin Setup Complete!
+              ============================================================
+              Username: admin
+              Password: {{ admin_password }}
+              
+              Access from desktop:
+              {% for ip in node_ips.stdout.split() %}
+              - http://{{ ip }}:30080/auth
+              {% endfor %}
+              
+              Admin Console:
+              {% for ip in node_ips.stdout.split() %}
+              - http://{{ ip }}:30080/auth/admin
+              {% endfor %}
+              
+              Credentials saved to: {{ backup_dir }}/keycloak-admin-credentials.txt
+              ============================================================
 
     - name: Display backup files and checksums (if created)
       block:
