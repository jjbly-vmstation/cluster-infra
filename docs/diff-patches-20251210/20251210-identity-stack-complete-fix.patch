diff --git a/ansible/playbooks/identity-deploy-and-handover.yml b/ansible/playbooks/identity-deploy-and-handover.yml
index 79cb02b..8f6463f 100644
--- a/ansible/playbooks/identity-deploy-and-handover.yml
+++ b/ansible/playbooks/identity-deploy-and-handover.yml
@@ -21,6 +21,7 @@
     keycloak_manifest: "{{ repo_root }}/manifests/keycloak.yaml"
     keycloak_helm_chart: "codecentric/keycloak"
     keycloak_values_file: "{{ repo_root }}/helm/keycloak-values.yaml"
+    postgresql_manifest: "{{ repo_root }}/manifests/identity/postgresql-statefulset.yaml"
     
     # Storage manifests for Keycloak PostgreSQL
     storage_class_manifest: "{{ repo_root }}/manifests/identity/storage-class-manual.yaml"
@@ -64,11 +65,11 @@
     # Increased from 180s to 300s to accommodate PostgreSQL initialization on slower systems
     rollout_wait_timeout: 120
     
-    # PostgreSQL image configuration
+    # PostgreSQL image configuration  
     postgresql_image_registry: docker.io
     postgresql_image_repository: postgres
     postgresql_image_tag: "11"
-    # PostgreSQL user/group IDs for the official postgres image (UID/GID 999)
+    # PostgreSQL user/group IDs for the postgres:11 image (UID/GID 999)
     # This must match the securityContext in keycloak-values.yaml
     postgresql_uid: "999"
     postgresql_gid: "999"
@@ -99,6 +100,7 @@
           register: infra_node_detect
           changed_when: false
           failed_when: false
+          become: true
 
         - name: Fallback to first schedulable node if no control-plane found
           shell: |
@@ -106,6 +108,7 @@
           register: infra_node_fallback
           changed_when: false
           when: infra_node_detect.stdout == ""
+          become: true
 
         - name: Set infra_node variable
           set_fact:
@@ -130,6 +133,7 @@
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       changed_when: false
+      become: true
 
     - name: Create identity data directories for persistent storage
       file:
@@ -151,6 +155,7 @@
       register: uncordon_result
       failed_when: false
       changed_when: false
+      become: true
 
     - name: Deploy StorageClass for Keycloak PostgreSQL (idempotent)
       shell: kubectl apply -f {{ storage_class_manifest }}
@@ -158,6 +163,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: storage_class_apply
       changed_when: "'created' in storage_class_apply.stdout or 'configured' in storage_class_apply.stdout"
+      become: true
 
     - name: Deploy PersistentVolume for Keycloak PostgreSQL (idempotent)
       shell: kubectl apply -f {{ keycloak_pv_manifest }}
@@ -165,6 +171,17 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_pv_apply
       changed_when: "'created' in keycloak_pv_apply.stdout or 'configured' in keycloak_pv_apply.stdout"
+      become: true
+
+    - name: Clear claimRef from Keycloak PostgreSQL PV if it's Released
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch pv keycloak-postgresql-pv 
+        --type=json -p='[{"op":"remove","path":"/spec/claimRef"}]'
+      when: keycloak_pv_apply.rc == 0
+      register: pv_claim_clear
+      failed_when: false
+      changed_when: "'patched' in pv_claim_clear.stdout"
+      become: true
 
     - name: Optionally fix hostPath ownership for PostgreSQL via privileged Job
       when: (enable_postgres_chown | default(false) | bool) or (identity_force_replace | default(false) | bool)
@@ -209,6 +226,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: chown_job_apply
           changed_when: "'created' in chown_job_apply.stdout or 'configured' in chown_job_apply.stdout"
+          become: true
 
         - name: Wait for chown Job completion
           shell: kubectl -n {{ namespace_identity }} wait --for=condition=complete job/postgres-chown-job --timeout=120s
@@ -216,6 +234,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: chown_job_wait
           failed_when: chown_job_wait.rc != 0
+          become: true
 
         - name: Delete chown Job
           shell: kubectl -n {{ namespace_identity }} delete job/postgres-chown-job --ignore-not-found
@@ -223,6 +242,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           changed_when: false
           failed_when: false
+          become: true
 
     - name: Optionally fix hostPath ownership for FreeIPA via privileged Job
       when: (enable_freeipa_chown | default(false) | bool) or (identity_force_replace | default(false) | bool)
@@ -267,6 +287,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: freeipa_chown_job_apply
           changed_when: "'created' in freeipa_chown_job_apply.stdout or 'configured' in freeipa_chown_job_apply.stdout"
+          become: true
 
         - name: Wait for FreeIPA chown Job completion
           shell: kubectl -n {{ namespace_identity }} wait --for=condition=complete job/freeipa-chown-job --timeout=120s
@@ -274,6 +295,7 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           register: freeipa_chown_job_wait
           failed_when: freeipa_chown_job_wait.rc != 0
+          become: true
 
         - name: Delete FreeIPA chown Job
           shell: kubectl -n {{ namespace_identity }} delete job/freeipa-chown-job --ignore-not-found
@@ -281,23 +303,25 @@
             KUBECONFIG: /etc/kubernetes/admin.conf
           changed_when: false
           failed_when: false
+          become: true
 
     - name: Check if FreeIPA manifest exists
       stat:
         path: "{{ freeipa_manifest }}"
       register: freeipa_manifest_stat
 
+    - name: Ensure backup directory exists upfront (root-owned)
+      file:
+        path: "{{ backup_dir }}"
+        state: directory
+        owner: root
+        group: root
+        mode: '0700'
+      become: true
+
     - name: "Optional: Backup and remove existing identity pods before redeploy (controlled)"
       when: identity_force_replace | default(false) | bool and identity_backup_before_replace | default(true) | bool
       block:
-        - name: Create backup directory on controller (root-owned)
-          file:
-            path: "{{ backup_dir }}"
-            state: directory
-            owner: root
-            group: root
-            mode: '0700'
-          become: true
 
         - name: Get current timestamp for backup filename
           shell: date -u +%Y%m%dT%H%M%SZ
@@ -366,28 +390,33 @@
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts freeipa --replicas=0 --timeout=60s || true
           register: scale_down_freeipa
           failed_when: false
+          become: true
 
         - name: Scale down Keycloak StatefulSet to 0 (if present)
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak --replicas=0 --timeout=60s || true
           register: scale_down_keycloak
           failed_when: false
+          become: true
 
         - name: Scale down PostgreSQL StatefulSet to 0 (if present)
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=0 --timeout=60s || true
           register: scale_down_postgres
           failed_when: false
+          become: true
 
         - name: Delete any remaining identity pods (force removal)
           shell: >-
             KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} delete pod --all --ignore-not-found --grace-period=10 --timeout=60s || true
           register: delete_identity_pods
           failed_when: false
+          become: true
 
         - name: Wait for identity pods to be fully removed
           shell: >-
             KUBECONFIG=/etc/kubernetes/admin.conf bash -c "for i in {1..60}; do if [ -z \"$(kubectl -n {{ namespace_identity }} get pods -o name 2>/dev/null)\" ]; then exit 0; fi; sleep 1; done; exit 1"
           register: wait_for_pod_removal
           failed_when: wait_for_pod_removal.rc != 0
+          become: true
 
     - name: Deploy FreeIPA from manifest (if available)
       shell: kubectl apply -f {{ freeipa_manifest }}
@@ -396,6 +425,17 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: freeipa_apply
       changed_when: "'created' in freeipa_apply.stdout or 'configured' in freeipa_apply.stdout"
+      become: true
+
+    - name: Clear claimRef from FreeIPA PV if it's Released
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch pv freeipa-data-pv 
+        --type=json -p='[{"op":"remove","path":"/spec/claimRef"}]'
+      when: freeipa_apply is defined and freeipa_apply.rc == 0
+      register: freeipa_pv_claim_clear
+      failed_when: false
+      changed_when: "'patched' in freeipa_pv_claim_clear.stdout"
+      become: true
 
     - name: Set nodeSelector for FreeIPA StatefulSet
       shell: |
@@ -405,6 +445,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: freeipa_node_selector
       failed_when: false
+      become: true
 
     - name: Inform about missing FreeIPA manifest
       debug:
@@ -423,6 +464,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_apply
       changed_when: "'created' in keycloak_apply.stdout or 'configured' in keycloak_apply.stdout"
+      become: true
 
     - name: Check if Keycloak values file has placeholder passwords
       shell: |
@@ -468,12 +510,20 @@
           ============================================================
       when: freeipa_password_check is defined and freeipa_password_check.stdout != ""
 
+    - name: Deploy PostgreSQL StatefulSet for Keycloak
+      shell: kubectl apply -f {{ postgresql_manifest }}
+      environment:
+        KUBECONFIG: /etc/kubernetes/admin.conf
+      register: postgresql_apply
+      changed_when: "'created' in postgresql_apply.stdout or 'configured' in postgresql_apply.stdout"
+      become: true
+
     - name: Check if Keycloak values file exists
       stat:
         path: "{{ keycloak_values_file }}"
       register: keycloak_values_stat
 
-    - name: Install/upgrade Keycloak via Helm
+    - name: Install/upgrade Keycloak via Helm (PostgreSQL disabled)
       shell: >-
         helm repo add codecentric https://codecentric.github.io/helm-charts >/dev/null 2>&1 || true;
         helm repo update >/dev/null 2>&1;
@@ -481,18 +531,38 @@
         -n {{ namespace_identity }} 
         -f {{ keycloak_values_file }} 
         --create-namespace 
-        --set postgresql.image.registry={{ postgresql_image_registry }}
-        --set postgresql.image.repository={{ postgresql_image_repository }}
-        --set postgresql.image.tag={{ postgresql_image_tag }}
-        --set postgresql.image.pullPolicy=IfNotPresent
+        --set postgresql.enabled=false
         --set keycloak.nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
-        --set postgresql.nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
       when: keycloak_values_stat.stat.exists
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: keycloak_helm_install
+      become: true
 
+    - name: Patch Keycloak StatefulSet with PostgreSQL environment variables
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch statefulset keycloak -n {{ namespace_identity }} 
+        --type=json -p='[{"op":"add","path":"/spec/template/spec/containers/0/env","value":[
+        {"name":"DB_VENDOR","value":"postgres"},
+        {"name":"DB_ADDR","value":"keycloak-postgresql"},
+        {"name":"DB_PORT","value":"5432"},
+        {"name":"DB_DATABASE","value":"keycloak"},
+        {"name":"DB_USER","value":"keycloak"},
+        {"name":"DB_PASSWORD","value":"CHANGEME_DB_PASSWORD"}
+        ]}]'
+      register: keycloak_env_patch
+      failed_when: false
+      changed_when: "'patched' in keycloak_env_patch.stdout"
+      become: true
 
+    - name: Patch Keycloak StatefulSet with nodeSelector for masternode
+      shell: >-
+        KUBECONFIG=/etc/kubernetes/admin.conf kubectl patch statefulset keycloak -n {{ namespace_identity }} 
+        -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"{{ infra_node }}"}}}}}'
+      register: keycloak_node_patch
+      failed_when: false
+      changed_when: "'patched' in keycloak_node_patch.stdout"
+      become: true
 
     - name: Wait for Keycloak PostgreSQL PVC to be created
       shell: >-
@@ -502,6 +572,7 @@
       retries: 30
       delay: 2
       failed_when: false
+      become: true
 
     - name: Check Keycloak PostgreSQL PVC status
       shell: >-
@@ -509,6 +580,7 @@
       register: keycloak_postgres_pvc_phase
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Find Available PVs labeled for Keycloak PostgreSQL
       shell: >-
@@ -516,6 +588,7 @@
       register: keycloak_postgres_available_pvs
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Bind an Available PV to Keycloak PVC when PVC is Pending
       shell: >-
@@ -528,6 +601,7 @@
       register: keycloak_pv_bind
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
+      become: true
 
     - name: Log PV binding result
       debug:
@@ -540,6 +614,7 @@
       register: keycloak_postgres_pvc_verify
       changed_when: false
       failed_when: false
+      become: true
 
     - name: Display PVC status
       debug:
@@ -559,6 +634,7 @@
             -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
           register: keycloak_pg_rollout
           failed_when: keycloak_pg_rollout.rc != 0
+          become: true
 
       rescue:
         - name: Handle PostgreSQL rollout timeout
@@ -579,6 +655,12 @@
                   kubectl -n {{ namespace_identity }} describe $pod || true
                 done
                 echo ""
+                echo "=== PostgreSQL Logs ==="
+                for pod in $(kubectl -n {{ namespace_identity }} get pods -l app.kubernetes.io/name=postgresql -o name); do
+                  echo "--- Logs: $pod ---"
+                  kubectl -n {{ namespace_identity }} logs $pod --all-containers --tail=100 || true
+                done
+                echo ""
                 echo "=== PVC Status ==="
                 kubectl -n {{ namespace_identity }} get pvc -o yaml || true
                 echo ""
@@ -590,6 +672,7 @@
               register: pg_diagnostics
               environment:
                 KUBECONFIG: /etc/kubernetes/admin.conf
+              become: true
 
             - name: Save PostgreSQL diagnostics
               copy:
@@ -621,10 +704,12 @@
                 - name: Scale down PostgreSQL StatefulSet
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=0 --timeout=60s
                   failed_when: false
+                  become: true
 
                 - name: Delete PostgreSQL pods
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} delete pod -l app.kubernetes.io/name=postgresql --ignore-not-found --grace-period=10
                   failed_when: false
+                  become: true
 
                 - name: Wait for pod removal
                   pause:
@@ -633,12 +718,14 @@
                 - name: Scale up PostgreSQL StatefulSet
                   shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_identity }} scale sts keycloak-postgresql --replicas=1
                   failed_when: false
+                  become: true
 
                 - name: Wait for PostgreSQL rollout after recovery
                   shell: >-
                     KUBECONFIG=/etc/kubernetes/admin.conf kubectl rollout status statefulset/keycloak-postgresql 
                     -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
                   register: pg_rollout_after_recovery
+                  become: true
 
             - name: Fail with diagnostics notice if recovery not attempted
               fail:
@@ -653,6 +740,7 @@
             -n {{ namespace_identity }} --timeout={{ rollout_wait_timeout }}s
           register: keycloak_rollout
           failed_when: keycloak_rollout.rc != 0
+          become: true
 
       rescue:
         - name: Handle Keycloak rollout timeout
@@ -673,11 +761,18 @@
                   kubectl -n {{ namespace_identity }} describe $pod || true
                 done
                 echo ""
+                echo "=== Keycloak Logs ==="
+                for pod in $(kubectl -n {{ namespace_identity }} get pods -l app.kubernetes.io/name=keycloak -o name); do
+                  echo "--- Logs: $pod ---"
+                  kubectl -n {{ namespace_identity }} logs $pod --all-containers --tail=100 || true
+                done
+                echo ""
                 echo "=== Events ==="
                 kubectl -n {{ namespace_identity }} get events --sort-by=.metadata.creationTimestamp || true
               register: kc_diagnostics
               environment:
                 KUBECONFIG: /etc/kubernetes/admin.conf
+              become: true
 
             - name: Save Keycloak diagnostics
               copy:
@@ -699,6 +794,7 @@
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: certmgr_crds
       changed_when: "'created' in certmgr_crds.stdout or 'configured' in certmgr_crds.stdout"
+      become: true
 
     - name: Add cert-manager Helm repo
       shell: |
@@ -715,6 +811,7 @@
         --set installCRDs=false
         --set nodeSelector."kubernetes\.io/hostname"={{ infra_node }}
       register: certmgr_helm
+      become: true
 
     - name: Wait for cert-manager deployments to be ready (with diagnostics on failure)
       block:
@@ -725,6 +822,7 @@
           failed_when: certmgr_rollout_controller.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Wait for cert-manager webhook rollout
           shell: >-
@@ -733,6 +831,7 @@
           failed_when: certmgr_rollout_webhook.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Wait for cert-manager cainjector rollout
           shell: >-
@@ -741,6 +840,7 @@
           failed_when: certmgr_rollout_cainjector.rc != 0
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
       rescue:
         - name: Get UTC timestamp for diagnostics filename
@@ -752,6 +852,7 @@
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_cert_manager }} get pods -o wide
           register: certmgr_pods
           changed_when: false
+          become: true
 
         - name: Describe cert-manager namespace pods
           shell: |
@@ -763,22 +864,25 @@
           changed_when: false
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
-        - name: Collect cert-manager webhook logs (best-effort)
+        - name: Collect cert-manager logs from all pods
           shell: |
-            set -e; for p in $(kubectl -n {{ namespace_cert_manager }} get pods -l app.kubernetes.io/component=webhook -o name 2>/dev/null || true); do
-              echo "=== logs $p ===";
-              kubectl -n {{ namespace_cert_manager }} logs $p --all-containers || true;
+            for p in $(kubectl -n {{ namespace_cert_manager }} get pods -o name 2>/dev/null || true); do
+              echo "=== Logs: $p ===";
+              kubectl -n {{ namespace_cert_manager }} logs $p --all-containers --tail=100 || true;
             done
           register: certmgr_logs
           changed_when: false
           environment:
             KUBECONFIG: /etc/kubernetes/admin.conf
+          become: true
 
         - name: Collect events in cert-manager namespace
           shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n {{ namespace_cert_manager }} get events --sort-by=.metadata.creationTimestamp
           register: certmgr_events
           changed_when: false
+          become: true
 
         - name: Save cert-manager diagnostics to root backup dir
           copy:
@@ -807,19 +911,21 @@
       register: crd_check
       failed_when: crd_check.rc != 0
       changed_when: false
+      become: true
 
     - name: Ensure CA cert file exists
       stat:
         path: "{{ ca_cert_src }}"
       register: ca_cert_stat
 
-    - name: Ensure backup directory exists (root-owned)
+    - name: Ensure backup directory exists (root-owned) before CA backup
       file:
         path: "{{ backup_dir }}"
         state: directory
         owner: root
         group: root
         mode: '0700'
+      become: true
 
     - name: Backup CA material to root backup (cert and key if present)
       shell: >-
@@ -831,12 +937,16 @@
       register: backup_run
       changed_when: backup_run.rc == 0
 
-    - name: Create or update Kubernetes Secret with CA
+    - name: Create or update Kubernetes Secret with CA (tls.crt and tls.key format for cert-manager)
       shell: >-
-        kubectl create secret generic {{ secret_name }} --namespace {{ namespace_cert_manager }} --from-file=ca.crt={{ ca_cert_src }} --dry-run=client -o yaml | kubectl apply -f -
+        kubectl create secret generic {{ secret_name }} --namespace {{ namespace_cert_manager }} 
+        --from-file=tls.crt={{ ca_cert_src }} 
+        --from-file=tls.key={{ ca_key_src }} 
+        --dry-run=client -o yaml | kubectl apply -f -
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: secret_apply
+      become: true
 
     - name: Render ClusterIssuer template
       template:
@@ -848,6 +958,7 @@
       environment:
         KUBECONFIG: /etc/kubernetes/admin.conf
       register: issuer_apply
+      become: true
 
     - name: Display backup files and checksums (if created)
       block:
diff --git a/helm/keycloak-values.yaml b/helm/keycloak-values.yaml
index 6fe5bdd..96149fd 100644
--- a/helm/keycloak-values.yaml
+++ b/helm/keycloak-values.yaml
@@ -22,6 +22,19 @@ replicaCount: 1
 keycloak:
   adminUser: "admin"
   adminPassword: "CHANGEME_ADMIN_PASSWORD"
+  extraEnv:
+    - name: DB_VENDOR
+      value: postgres
+    - name: DB_ADDR
+      value: keycloak-postgresql
+    - name: DB_PORT
+      value: "5432"
+    - name: DB_DATABASE
+      value: keycloak
+    - name: DB_USER
+      value: keycloak
+    - name: DB_PASSWORD
+      value: CHANGEME_DB_PASSWORD
 
 # Service exposure
 service:
@@ -31,10 +44,15 @@ service:
 # Use PostgreSQL (recommended for production)
 database:
   vendor: postgres
+  hostname: keycloak-postgresql
+  port: 5432
+  database: keycloak
+  username: keycloak
+  password: "CHANGEME_DB_PASSWORD"
 
-# PostgreSQL subchart configuration
+# PostgreSQL subchart configuration (disabled - we use standalone StatefulSet)
 postgresql:
-  enabled: true
+  enabled: false
   auth:
     username: keycloak
     password: "CHANGEME_DB_PASSWORD"
@@ -46,19 +64,15 @@ postgresql:
     repository: postgres
     tag: "11"
     pullPolicy: IfNotPresent
-  # The official postgres:11 image does not support the Bitnami volumePermissions
-  # initContainer pattern. Instead, the playbook pre-creates the hostPath directory
-  # with ownership set to UID 999 (postgres user in the official image).
+  # Disable volumePermissions since the playbook handles directory ownership
+  # The playbook pre-creates the hostPath with correct UID/GID (999:999)
   volumePermissions:
     enabled: false
-  # The official postgres:11 image runs as user 'postgres' (UID 999).
-  # Set fsGroup to 999 so Kubernetes adjusts group ownership on the volume mount,
-  # allowing the postgres process to write to the data directory.
+  # The postgres:11 image runs as root initially then switches to postgres (UID 999).
+  # Don't set runAsUser/fsGroup - let the image handle user switching.
   # NOTE: If changing the PostgreSQL image, verify the user ID and update both
-  # runAsUser/fsGroup here and postgresql_uid/postgresql_gid in the playbook.
-  securityContext:
-    runAsUser: 999  # Official postgres image user ID
-    fsGroup: 999    # Must match runAsUser
+  # postgresql_uid/postgresql_gid in the playbook.
+  securityContext: {}
   primary:
     persistence:
       enabled: true
diff --git a/kubespray b/kubespray
index 2cb8c85..2edf176 160000
--- a/kubespray
+++ b/kubespray
@@ -1 +1 @@
-Subproject commit 2cb8c8544fc5aeff7e1bc0865e65837dd6e8eaca
+Subproject commit 2edf176294632898d236bbf988b4d4602f913598
